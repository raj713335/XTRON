{"cells":[{"metadata":{"_uuid":"50bec9910540e00be6b8c30cde11fe8adfe12318"},"cell_type":"markdown","source":["> *Half a million children become ill with TB each year. There are 10 million children worldwide who had been orphaned because a parent died of TB.*<br>\n","> *Basic diagnosis of TB has not changed for more than a century. New genetic tests for TB make it possible to rapidly identify people who need TB treatment. But a simple quick test of the sort already available for diseases like HIV and malaria is needed urgently.* <br>\n",">*** ~ stoptb.org***"]},{"metadata":{"_uuid":"49347492fe47aa4c25705e64a6d9899c185935f8"},"cell_type":"markdown","source":["**Introduction**\n","\n","In this kernel we will build a model that can look at a chest x-ray and predict whether a person has TB or not. The model will be trained on a dataset of 800 images from two sources:\n","\n","- Shenzhen, China (Folder: ChinaSet_AllFiles)\n","- Montgomery, USA (Folder: Montgomery)\n","\n","**Results:**\n","\n","The dataset is quite small but by using a CNN and data augmentation, the final accuracy and F1 score that we get will be greater than 0.8. Because we need to use as many images as possible for training, the validation set will contain only 120 images. This is 15% of the data.\n","\n","With a small dataset and a very small validation set, will this model generalize well? I don't really know. However, I've deployed the model as a Flask web app so it can be tested. Using a web app is one way that a tool like this could - quickly and cheaply - be put in the hands of medical personnel that need it.\n","\n","***"]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":["from numpy.random import seed\n","seed(101)\n","from tensorflow import set_random_seed\n","set_random_seed(101)\n","\n","import pandas as pd\n","import numpy as np\n","\n","import tensorflow\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import categorical_crossentropy\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from tensorflow.keras.metrics import binary_accuracy\n","\n","import os\n","import cv2\n","\n","import imageio\n","import skimage\n","import skimage.io\n","import skimage.transform\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import itertools\n","import shutil\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7bbdd52c81188b8e9c528b88d9fd0da176bf4bc"},"cell_type":"code","source":["# Total number of images we want to have in each class\n","NUM_AUG_IMAGES_WANTED = 1000 \n","\n","# We will resize the images\n","IMAGE_HEIGHT = 96\n","IMAGE_WIDTH = 96"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25802a26e4afba8f6dc42754f7f61da4c8cfea5c"},"cell_type":"markdown","source":["### What files are available?"]},{"metadata":{"trusted":true,"_uuid":"699bb899bde433ba20fb0d086fa0f33a0f61a250"},"cell_type":"code","source":["os.listdir('../input')"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cdfb7348c70903bf3eb281c7573100cce8b3d72"},"cell_type":"markdown","source":["### What are the labels?"]},{"metadata":{"_uuid":"8ef8ffdc93d54089bd6cb35fc2fecfdf187a119c"},"cell_type":"markdown","source":["\n","The label is part of the file name.\n","\n","Example: *CHNCXR_0470__**1**.png*<br>\n","\n","**0** = Normal (No TB)<br>\n","**1** = TB<br>\n","\n","Each of the two datasets has a text file containing meta-data."]},{"metadata":{"_uuid":"5a285343c286be191aa827ff9b836b67821176a5"},"cell_type":"markdown","source":["### How many images are in each folder?\n","Note: In both the Mongomery and Shenzhen folders there is a non image file called 'Thumbs.db'"]},{"metadata":{"trusted":true,"_uuid":"54461212efed65ac377369a468c80e7d708010f4"},"cell_type":"code","source":["print(len(os.listdir('../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png')))\n","print(len(os.listdir('../input/Montgomery/MontgomerySet/CXR_png')))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b90854e07d495d9f945a0e40189fd32a0c32bff5"},"cell_type":"markdown","source":["### Create a Dataframe containing all images"]},{"metadata":{"trusted":true,"_uuid":"e9c9f40ffab35044641b0dc7d9b18609af1aa25e"},"cell_type":"code","source":["shen_image_list = os.listdir('../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png')\n","mont_image_list = os.listdir('../input/Montgomery/MontgomerySet/CXR_png')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"286a5b9125c6cca0ea989e00c0355326794a8ec1"},"cell_type":"code","source":["# put the images into dataframes\n","df_shen = pd.DataFrame(shen_image_list, columns=['image_id'])\n","df_mont = pd.DataFrame(mont_image_list, columns=['image_id'])\n","\n","# remove the 'Thunbs.db' line\n","df_shen = df_shen[df_shen['image_id'] != 'Thumbs.db']\n","df_mont = df_mont[df_mont['image_id'] != 'Thumbs.db']\n","\n","# Reset the index or this will cause an error later\n","df_shen.reset_index(inplace=True, drop=True)\n","df_mont.reset_index(inplace=True, drop=True)\n","\n","print(df_shen.shape)\n","print(df_mont.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"492ed5b3c8ab34a1f1511ad489c978b29267ddb6"},"cell_type":"code","source":["df_shen.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5c24782c7ba338a58e0f1f43f9c5bdd1babc3f3"},"cell_type":"code","source":["df_mont.head()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27cfbb4b833ad118b19806bd644f96aa02533651"},"cell_type":"markdown","source":["### Assign labels to the images"]},{"metadata":{"trusted":true,"_uuid":"ebd4f30e06ceb2ae6bf992325501998420ac61e5"},"cell_type":"code","source":["# Function to select the 4th index from the end of the string (file name)\n","# example: CHNCXR_0470_1.png --> 1 is the label, meaning TB is present.\n","\n","def extract_target(x):\n","    target = int(x[-5])\n","    if target == 0:\n","        return 'Normal'\n","    if target == 1:\n","        return 'Tuberculosis'"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22f5fc095e1cba8d4c3535b4387bbf59b4a082ab"},"cell_type":"code","source":["# Assign the target labels\n","\n","df_shen['target'] = df_shen['image_id'].apply(extract_target)\n","\n","df_mont['target'] = df_mont['image_id'].apply(extract_target)\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfbd53b7f8ea1929952ffed6221b380012618e32"},"cell_type":"markdown","source":["### Check the class distribution"]},{"metadata":{"trusted":true,"_uuid":"e18560bf69d3dfc0c4772e7c79bb119fd2eb634b"},"cell_type":"code","source":["# Shenzen Dataset\n","\n","df_shen['target'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a0f3d5edbe37b2dc9fd46189e3fdcb63da4feb1"},"cell_type":"code","source":["# Montgomery Dataset\n","\n","df_mont['target'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6efe2e5de99c4bf92079b1a7d0b892d30fc9d518"},"cell_type":"markdown","source":["### Display a ramdom sample of images from each dataset by target"]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1c5143f227da4262eafce8cf0210a02c8072fb8e"},"cell_type":"code","source":["# source: https://www.kaggle.com/gpreda/honey-bee-subspecies-classification\n","\n","def draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n","    \n","    \"\"\"\n","    Give a column in a dataframe,\n","    this function takes a sample of each class and displays that\n","    sample on one row. The sample size is the same as figure_cols which\n","    is the number of columns in the figure.\n","    Because this function takes a random sample, each time the function is run it\n","    displays different images.\n","    \"\"\"\n","    \n","\n","    categories = (df.groupby([col_name])[col_name].nunique()).index\n","    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n","                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n","    # draw a number of images for each location\n","    for i, cat in enumerate(categories):\n","        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n","        for j in range(0,figure_cols):\n","            file=IMAGE_PATH + sample.iloc[j]['image_id']\n","            im=imageio.imread(file)\n","            ax[i, j].imshow(im, resample=True, cmap='gray')\n","            ax[i, j].set_title(cat, fontsize=14)  \n","    plt.tight_layout()\n","    plt.show()\n","    "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd38bcfb5839975e4fee9e70b93d42c29c1b5d2e"},"cell_type":"code","source":["# Shenzen Dataset\n","\n","IMAGE_PATH = '../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png/' \n","\n","draw_category_images('target',4, df_shen, IMAGE_PATH)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d6b11f3b0f26f1e22993896a20cfd1d90d5c5d1"},"cell_type":"code","source":["# Montgomery Dataset\n","\n","IMAGE_PATH = '../input/Montgomery/MontgomerySet/CXR_png/'\n","\n","draw_category_images('target',4, df_mont, IMAGE_PATH)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21d2d3c8c75e6ac7a9ba9480b222c9b9ec3e60fa"},"cell_type":"markdown","source":["### What is the shape of each image and what are its max and min pixel values?\n","Let's include all this info in the dataframes we created above."]},{"metadata":{"trusted":true,"_uuid":"4777cecabf520c32ace90e8e96cad895663a89e8","_kg_hide-input":true},"cell_type":"code","source":["def read_image_sizes(file_name):\n","    \"\"\"\n","    1. Get the shape of the image\n","    2. Get the min and max pixel values in the image.\n","    Getting pixel values will tell if any pre-processing has been done.\n","    3. This info will be added to the original dataframe.\n","    \"\"\"\n","    image = cv2.imread(IMAGE_PATH + file_name)\n","    max_pixel_val = image.max()\n","    min_pixel_val = image.min()\n","    \n","    # image.shape[2] represents the number of channels: (height, width, num_channels).\n","    # Here we are saying: If the shape does not have a value for num_channels (height, width)\n","    # then assign 1 to the number of channels.\n","    if len(image.shape) > 2: # i.e. more than two numbers in the tuple\n","        output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val]\n","    else:\n","        output = [image.shape[0], image.shape[1], 1, max_pixel_val, min_pixel_val]\n","    return output\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1651542ea66843ca948042a4047c89bb405d9009"},"cell_type":"code","source":["IMAGE_PATH = '../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png/'\n","\n","m = np.stack(df_shen['image_id'].apply(read_image_sizes))\n","df = pd.DataFrame(m,columns=['w','h','c','max_pixel_val','min_pixel_val'])\n","df_shen = pd.concat([df_shen,df],axis=1, sort=False)\n","\n","df_shen.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad6701c7a0c292293c71b7487c4597220816e35a"},"cell_type":"code","source":["IMAGE_PATH = '../input/Montgomery/MontgomerySet/CXR_png/'\n","\n","m = np.stack(df_mont['image_id'].apply(read_image_sizes))\n","df = pd.DataFrame(m,columns=['w','h','c','max_pixel_val','min_pixel_val'])\n","df_mont = pd.concat([df_mont,df],axis=1, sort=False)\n","\n","df_mont.head()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fffaaebc9c51867063fe3859e48c9b913adce501"},"cell_type":"markdown","source":["### How many channels do the images in each dataset have?"]},{"metadata":{"trusted":true,"_uuid":"a9dad0671ac757d75d0524080ba7b1013a535b37"},"cell_type":"code","source":["df_shen['c'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdffc711f9b7a4346569d1c79a45704c2d660ee6"},"cell_type":"code","source":["df_mont['c'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e661e622490e1a574bb9da2a24b9d08ad3e05f8f"},"cell_type":"markdown","source":["We see that all images have 3 channels."]},{"metadata":{"_uuid":"1da4226777aefe65b1bb3430208ea91ea7ca7d9a"},"cell_type":"markdown","source":["### Create the Train and Val Sets"]},{"metadata":{"trusted":true,"_uuid":"4414c95eb7f2729d54f4a0e422061a45f209dfec"},"cell_type":"code","source":["df_mont['target'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c5b5485b90e1520629f88093ebe2e0fbb5c3a98"},"cell_type":"code","source":["### Combine the two dataframes and shuffle\n","\n","df_data = pd.concat([df_shen, df_mont], axis=0).reset_index(drop=True)\n","\n","df_data = shuffle(df_data)\n","\n","\n","df_data.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cd4d67bbe66b84374d3b64c5651f901602f4603"},"cell_type":"code","source":["# Create a new column called 'labels' that maps the classes to binary values.\n","df_data['labels'] = df_data['target'].map({'Normal':0, 'Tuberculosis':1})"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"470db2e9a0f8ea62bb434de381dc6e9d065a3a30"},"cell_type":"code","source":["df_data.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15ba9792e6a370b7560330af15b3cfe21185c1cb"},"cell_type":"code","source":["# train_test_split\n","\n","y = df_data['labels']\n","\n","df_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\n","\n","print(df_train.shape)\n","print(df_val.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2b267a97f465a4fc7803b072689eb661890168"},"cell_type":"code","source":["df_train['target'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"392c0eea00be8e43a6e55438d1458650e842030b"},"cell_type":"code","source":["df_val['target'].value_counts()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba17dd34b75367fc61df6634d51dac94c3ab4951"},"cell_type":"markdown","source":["### Create a Directory Structure"]},{"metadata":{"trusted":true,"_uuid":"ff8acc2e92a1b1b5002d6e1bf9a1180c3256f19d"},"cell_type":"code","source":["# Create a new directory\n","base_dir = 'base_dir'\n","os.mkdir(base_dir)\n","\n","\n","#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n","\n","# now we create 2 folders inside 'base_dir':\n","\n","# train\n","    # Normal\n","    # Tuberculosis\n","\n","# val\n","    # Normal\n","    # Tuberculosis\n","\n","\n","# create a path to 'base_dir' to which we will join the names of the new folders\n","# train_dir\n","train_dir = os.path.join(base_dir, 'train_dir')\n","os.mkdir(train_dir)\n","\n","# val_dir\n","val_dir = os.path.join(base_dir, 'val_dir')\n","os.mkdir(val_dir)\n","\n","\n","# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n","# Inside each folder we create seperate folders for each class\n","\n","# create new folders inside train_dir\n","Normal = os.path.join(train_dir, 'Normal')\n","os.mkdir(Normal)\n","Tuberculosis = os.path.join(train_dir, 'Tuberculosis')\n","os.mkdir(Tuberculosis)\n","\n","\n","# create new folders inside val_dir\n","Normal = os.path.join(val_dir, 'Normal')\n","os.mkdir(Normal)\n","Tuberculosis = os.path.join(val_dir, 'Tuberculosis')\n","os.mkdir(Tuberculosis)\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a2e56340ba18f3b63c1b129fd995fecfadaa21d"},"cell_type":"markdown","source":["### Transfer the images into the folders"]},{"metadata":{"trusted":true,"_uuid":"e84c8a9642b030094b1888af3299063f883112a6"},"cell_type":"code","source":["# Set the image_id as the index in df_data\n","df_data.set_index('image_id', inplace=True)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afb8969a9ee75c13bddc808a4bcc326611baaaaf"},"cell_type":"code","source":["# Get a list of images in each of the two folders\n","folder_1 = os.listdir('../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png')\n","folder_2 = os.listdir('../input/Montgomery/MontgomerySet/CXR_png')\n","\n","# Get a list of train and val images\n","train_list = list(df_train['image_id'])\n","val_list = list(df_val['image_id'])\n","\n","\n","\n","# Transfer the train images\n","\n","for image in train_list:\n","    \n","    fname = image\n","    label = df_data.loc[image,'target']\n","    \n","    if fname in folder_1:\n","        # source path to image\n","        src = os.path.join('../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png', fname)\n","        # destination path to image\n","        dst = os.path.join(train_dir, label, fname)\n","        \n","        image = cv2.imread(src)\n","        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        # save the image at the destination\n","        cv2.imwrite(dst, image)\n","        #shutil.copyfile(src, dst)\n","\n","    if fname in folder_2:\n","        # source path to image\n","        src = os.path.join('../input/Montgomery/MontgomerySet/CXR_png', fname)\n","        # destination path to image\n","        dst = os.path.join(train_dir, label, fname)\n","        \n","        image = cv2.imread(src)\n","        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        # save the image at the destination\n","        cv2.imwrite(dst, image)\n","        \n","        # copy the image from the source to the destination\n","        #shutil.copyfile(src, dst)\n","\n","\n","# Transfer the val images\n","\n","for image in val_list:\n","    \n","    fname = image\n","    label = df_data.loc[image,'target']\n","    \n","    if fname in folder_1:\n","        # source path to image\n","        src = os.path.join('../input/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png', fname)\n","        # destination path to image\n","        dst = os.path.join(val_dir, label, fname)\n","        \n","        image = cv2.imread(src)\n","        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        # save the image at the destination\n","        cv2.imwrite(dst, image)\n","        \n","        # copy the image from the source to the destination\n","        #shutil.copyfile(src, dst)\n","\n","    if fname in folder_2:\n","        # source path to image\n","        src = os.path.join('../input/Montgomery/MontgomerySet/CXR_png', fname)\n","        # destination path to image\n","        dst = os.path.join(val_dir, label, fname)\n","        \n","        image = cv2.imread(src)\n","        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n","        # save the image at the destination\n","        cv2.imwrite(dst, image)\n","        \n","        # copy the image from the source to the destination\n","        #shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71532bfc32608289b1f773ffdbc8a7cea1bfb94c"},"cell_type":"code","source":["# check how many train images we have in each folder\n","\n","print(len(os.listdir('base_dir/train_dir/Normal')))\n","print(len(os.listdir('base_dir/train_dir/Tuberculosis')))\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897e9df543bb65b47bb00019dc681125ca08ee5d"},"cell_type":"code","source":["# check how many val images we have in each folder\n","\n","print(len(os.listdir('base_dir/val_dir/Normal')))\n","print(len(os.listdir('base_dir/val_dir/Tuberculosis')))\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76f9499d5fe8e7cb4954c11fe9a592e4d6e4776d"},"cell_type":"markdown","source":["### Copy the train images into aug_dir\n","aug_dir is where we temporarily store images from a given class before feeding them into the generator for augmentation.Â¶\n","\n","We will not be augmenting on the fly. We will create augmented images, store them in folders together with the raw images and then feed these into the generators. I found that working this way makes the training process run faster."]},{"metadata":{"trusted":true,"_uuid":"395839a0b38b954aee0b8fca348a07909f502432"},"cell_type":"code","source":["class_list = ['Normal','Tuberculosis']\n","\n","for item in class_list:\n","    \n","    # We are creating temporary directories here because we delete these directories later.\n","    # create a base dir\n","    aug_dir = 'aug_dir'\n","    os.mkdir(aug_dir)\n","    # create a dir within the base dir to store images of the same class\n","    img_dir = os.path.join(aug_dir, 'img_dir')\n","    os.mkdir(img_dir)\n","\n","    # Choose a class\n","    img_class = item\n","\n","    # list all images in that directory\n","    img_list = os.listdir('base_dir/train_dir/' + img_class)\n","\n","    # Copy images from the class train dir to the img_dir e.g. class 'Normal'\n","    for fname in img_list:\n","            # source path to image\n","            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n","            # destination path to image\n","            dst = os.path.join(img_dir, fname)\n","            # copy the image from the source to the destination\n","            shutil.copyfile(src, dst)\n","\n","\n","    # point to a dir containing the images and not to the images themselves\n","    path = aug_dir\n","    save_path = 'base_dir/train_dir/' + img_class\n","\n","    # Create a data generator\n","    datagen = ImageDataGenerator(\n","        rotation_range=10,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","\n","    batch_size = 50\n","\n","    aug_datagen = datagen.flow_from_directory(path,\n","                                           save_to_dir=save_path,\n","                                           save_format='png',\n","                                                    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n","                                                    batch_size=batch_size)\n","    \n","    \n","    # Generate the augmented images and add them to the training folders\n","    \n","    \n","    num_files = len(os.listdir(img_dir))\n","    \n","    # this creates a similar amount of images for each class\n","    num_batches = int(np.ceil((NUM_AUG_IMAGES_WANTED-num_files)/batch_size))\n","\n","    # run the generator and create augmented images\n","    for i in range(0,num_batches):\n","\n","        imgs, labels = next(aug_datagen)\n","        \n","    # delete temporary directory with the raw image files\n","    shutil.rmtree('aug_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"143ee5937df6cbd0cd673cd70e0b17be85453bdb"},"cell_type":"code","source":["# Check how many train images we now have in each folder.\n","# This is the original images plus the augmented images.\n","\n","print(len(os.listdir('base_dir/train_dir/Normal')))\n","print(len(os.listdir('base_dir/train_dir/Tuberculosis')))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a1e42fd565256d1f7f1b2a7c9ab1756680ed25a"},"cell_type":"code","source":["# Check how many val images we have in each folder.\n","\n","print(len(os.listdir('base_dir/val_dir/Normal')))\n","print(len(os.listdir('base_dir/val_dir/Tuberculosis')))"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91a023f302fc8c994235d6f4035cb22ae541a3c3"},"cell_type":"markdown","source":["### Visualize a batch of augmented images"]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e1ffd40b1eae248776a0c70d863a93dfd5fd99a6"},"cell_type":"code","source":["# plots images with labels within jupyter notebook\n","# source: https://github.com/smileservices/keras_utils/blob/master/utils.py\n","\n","def plots(ims, figsize=(20,10), rows=5, interp=False, titles=None): # 12,6\n","    if type(ims[0]) is np.ndarray:\n","        ims = np.array(ims).astype(np.uint8)\n","        if (ims.shape[-1] != 3):\n","            ims = ims.transpose((0,2,3,1))\n","    f = plt.figure(figsize=figsize)\n","    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n","    for i in range(len(ims)):\n","        sp = f.add_subplot(rows, cols, i+1)\n","        sp.axis('Off')\n","        if titles is not None:\n","            sp.set_title(titles[i], fontsize=16)\n","        plt.imshow(ims[i], interpolation=None if interp else 'none')\n","        \n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b60712c6405c98634a97b1b96bc6516329e279ae"},"cell_type":"code","source":["plots(imgs, titles=None) # titles=labels will display the image labels"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef780ac9a9fc89b4ff4f042593eb68992f354a1d"},"cell_type":"code","source":["# End of Data Preparation\n","### ===================================================================================== ###\n","# Start of Model Building\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8dce940ee8a7a42aacb062e4c6b5a4a54dba58f"},"cell_type":"markdown","source":["### Set Up the Generators"]},{"metadata":{"trusted":true,"_uuid":"ef4fe7be09f11ff4badfd22d5fd5e03f8521ed58"},"cell_type":"code","source":["train_path = 'base_dir/train_dir'\n","valid_path = 'base_dir/val_dir'\n","\n","num_train_samples = len(df_train)\n","num_val_samples = len(df_val)\n","train_batch_size = 10\n","val_batch_size = 10\n","\n","\n","train_steps = np.ceil(num_train_samples / train_batch_size)\n","val_steps = np.ceil(num_val_samples / val_batch_size)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68fbd9d5fbb80859a82f94a12e335ce05a93bd51"},"cell_type":"code","source":["\n","datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","train_gen = datagen.flow_from_directory(train_path,\n","                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n","                                        batch_size=train_batch_size,\n","                                        class_mode='categorical')\n","\n","val_gen = datagen.flow_from_directory(valid_path,\n","                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n","                                        batch_size=val_batch_size,\n","                                        class_mode='categorical')\n","\n","# Note: shuffle=False causes the test dataset to not be shuffled\n","test_gen = datagen.flow_from_directory(valid_path,\n","                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n","                                        batch_size=val_batch_size,\n","                                        class_mode='categorical',\n","                                        shuffle=False)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79da4d0a66a90cffe40580a596dd4d0e2bc45a9b"},"cell_type":"markdown","source":["### Create the Model Architecture"]},{"metadata":{"trusted":true,"_uuid":"b0d1a01f403a481e3f7ca715d46aa84a8aff7a4a","_kg_hide-output":true},"cell_type":"code","source":["# Source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n","\n","kernel_size = (3,3)\n","pool_size= (2,2)\n","first_filters = 32\n","second_filters = 64\n","third_filters = 128\n","\n","dropout_conv = 0.3\n","dropout_dense = 0.3\n","\n","\n","model = Sequential()\n","model.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n","                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n","model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n","model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n","model.add(MaxPooling2D(pool_size = pool_size)) \n","model.add(Dropout(dropout_conv))\n","\n","model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n","model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n","model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n","model.add(MaxPooling2D(pool_size = pool_size))\n","model.add(Dropout(dropout_conv))\n","\n","model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n","model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n","model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n","model.add(MaxPooling2D(pool_size = pool_size))\n","model.add(Dropout(dropout_conv))\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation = \"relu\"))\n","model.add(Dropout(dropout_dense))\n","model.add(Dense(2, activation = \"softmax\"))\n","\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75cfc4fcb8dd3408d1c4fcf8cd85e0e2f5b611d7"},"cell_type":"markdown","source":["### Train the Model"]},{"metadata":{"trusted":true,"_uuid":"9de9715f49a63b55775b10abd2f461b395e23b5d"},"cell_type":"code","source":["model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a746769db61563f226288eba9aa8a6584b9e8e0b","_kg_hide-output":true},"cell_type":"code","source":["filepath = \"model.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n","                             save_best_only=True, mode='max')\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n","                                   verbose=1, mode='max', min_lr=0.00001)\n","                              \n","                              \n","callbacks_list = [checkpoint, reduce_lr]\n","\n","history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n","                            validation_data=val_gen,\n","                            validation_steps=val_steps,\n","                            epochs=100, verbose=1,\n","                           callbacks=callbacks_list)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa15a8afda3593973726e9087cbd98073041c908"},"cell_type":"markdown","source":["### Evaluate the model using the val set"]},{"metadata":{"trusted":true,"_uuid":"70104420ec7f400cd06203f875dbeba30f4d8a96"},"cell_type":"code","source":["# get the metric names so we can use evaulate_generator\n","model.metrics_names"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"428bdf5b24ff8cef35012205c3f2eb37006fc9e9"},"cell_type":"code","source":["# Here the best epoch will be used.\n","\n","model.load_weights('model.h5')\n","\n","val_loss, val_acc = \\\n","model.evaluate_generator(test_gen, \n","                        steps=val_steps)\n","\n","print('val_loss:', val_loss)\n","print('val_acc:', val_acc)"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93556c9e4b6a188cf9cb67a6519c9bc365c60caf"},"cell_type":"markdown","source":["### Plot the Training Curves"]},{"metadata":{"trusted":true,"_uuid":"385da8ba94a1079d17909790716b295fc2737584","_kg_hide-input":true},"cell_type":"code","source":["# display the loss and accuracy curves\n","\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.figure()\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5636e76f23202dd1f2a27ace25e15e09619a5e4e"},"cell_type":"markdown","source":["### Create a Confusion Matrix"]},{"metadata":{"trusted":true,"_uuid":"2721a2820d76bdb39facec5d2fb3eebe868da45f"},"cell_type":"code","source":["# Get the labels of the test images.\n","\n","test_labels = test_gen.classes"],"execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"4605aad533f3afe0a6eb7dc869d0448d45bb26f6"},"cell_type":"code","source":["# We need these to plot the confusion matrix.\n","test_labels"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af6b19047f132f55ae24e207546d7cb8dc3db103"},"cell_type":"code","source":["# Print the label associated with each class\n","test_gen.class_indices"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"652d9d6aa51dc1818d1c5171212d10e141ad7de9"},"cell_type":"code","source":["# make a prediction\n","predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edf1866df4638ded26de2e0e3d2ba0f5e00e1ace"},"cell_type":"code","source":["predictions.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f570e8e5f07126e5361bbf92929d786e853a09","_kg_hide-input":true},"cell_type":"code","source":["# Source: Scikit Learn website\n","# http://scikit-learn.org/stable/auto_examples/\n","# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n","# selection-plot-confusion-matrix-py\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a537724473df19c74bb1bf6928f531dd0fcfdb3"},"cell_type":"code","source":["test_labels.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bb4323e931ff53081994bbf58e82b1ec93ab327"},"cell_type":"code","source":["# argmax returns the index of the max value in a row\n","cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec2058223eb30485898a12c0e904bb170c0aa884"},"cell_type":"code","source":["test_gen.class_indices"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba26c7e718df937a18aa2035c4ba883252e44c79"},"cell_type":"code","source":["# Define the labels of the class indices. These need to match the \n","# order shown above.\n","cm_plot_labels = ['Normal', 'Tuberculosis']\n","\n","plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2576c1144cfe93d66f3197396990f3e43addd499"},"cell_type":"markdown","source":["### Create a Classification Report"]},{"metadata":{"trusted":true,"_uuid":"db8cbeee5345aa7febf247707728a764e8e7c4b5"},"cell_type":"code","source":["# Get the filenames, labels and associated predictions\n","\n","# This outputs the sequence in which the generator processed the test images\n","test_filenames = test_gen.filenames\n","\n","# Get the true labels\n","y_true = test_gen.classes\n","\n","# Get the predicted labels\n","y_pred = predictions.argmax(axis=1)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91dcace7eb99aca310774b7a3a55535c9127ce55"},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Generate a classification report\n","\n","report = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n","\n","print(report)\n"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c3ff294228f20573abed1b00452c781f9afdbfe"},"cell_type":"markdown","source":["**Recall **= Given a class, will the classifier be able to detect it?<br>\n","**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n","**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n","\n","The F1 score is greater than 0.8. From the confusion matrix we see that our model has a  tendency to classify TB images as Normal, more so than to classify Normal images as TB. "]},{"metadata":{"trusted":true,"_uuid":"16d10a81e76ae51b3a72c492b498b543ccf12cce"},"cell_type":"markdown","source":["### Save Model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6f0bdce1a5a18d6c045eda2b58abb92e089d47","_kg_hide-output":true},"cell_type":"code","source":["# Delete the image data directory we created to prevent a Kaggle error.\n","# Kaggle allows a max of 500 files to be saved.\n","\n","shutil.rmtree('base_dir')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3686f78c6646cc64bd0c199f2e0d0f9dafebac49"},"cell_type":"markdown","source":["**Conclusion**\n","\n","Many thanks to Kevin Mader for making this dataset available on Kaggle.\n","\n","Thank you for reading."]},{"metadata":{"trusted":true,"_uuid":"c4bd4206e580522661299e470d163eb97d4e8f0c"},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}